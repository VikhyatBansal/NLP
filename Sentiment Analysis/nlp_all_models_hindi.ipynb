{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense,AveragePooling1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, GRU, Conv1D, MaxPooling1D, Flatten, Attention, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(r'D:\\CODING\\Python\\NLP\\END_SEM\\hindi_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_clean_hindi(text):\n",
    "    # Define the function to preprocess Hindi text by tokenizing\n",
    "    def preprocess_text_hindi(text):\n",
    "        tokens = text.split(\" \")\n",
    "        return tokens\n",
    "\n",
    "    # Define the function to remove non-Hindi characters while preserving spaces\n",
    "    def remove_non_hindi(text):\n",
    "        hindi_pattern = re.compile(\"[\\u0900-\\u097F\\s]+\")  # Unicode range for Hindi characters and space\n",
    "        hindi_text = hindi_pattern.findall(text)\n",
    "        cleaned_text = ''.join(hindi_text)\n",
    "        return cleaned_text\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = preprocess_text_hindi(text)\n",
    "\n",
    "    # Remove non-Hindi characters while preserving spaces\n",
    "    cleaned_text = list(map(remove_non_hindi,tokens))\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "data['text'] = data['text'].apply(preprocess_and_clean_hindi)\n",
    "\n",
    "data['label'] = data['sentiment'].map({'positive': 1, 'negative': 0,'neutral': 2})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(glove_file):\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        word_embeddings = {}\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            word_embeddings[word] = vector\n",
    "    return word_embeddings\n",
    "\n",
    "def generate_embedding_matrix(word_vector, embedding_model, embedding_type, max_length=200):\n",
    "    if embedding_type == 'glove':\n",
    "        embedding_dim = len(next(iter(embedding_model.values())))\n",
    "        embedding_matrix = np.zeros((max_length, embedding_dim))\n",
    "        for i, word in enumerate(word_vector):\n",
    "            if word in embedding_model:\n",
    "                embedding_matrix[i] = embedding_model[word]\n",
    "            if i >= max_length - 1:\n",
    "                break\n",
    "    else:\n",
    "        word_embeddings = []\n",
    "        for word in word_vector:\n",
    "            if word in embedding_model.wv:\n",
    "                word_embedding = embedding_model.wv[word]\n",
    "                word_embeddings.append(word_embedding)\n",
    "        embedding_matrix = np.array(word_embeddings)\n",
    "        if len(embedding_matrix) < max_length:\n",
    "            embedding_matrix = np.pad(embedding_matrix, ((0, max_length - len(embedding_matrix)), (0, 0)), mode='constant')\n",
    "        else:\n",
    "            embedding_matrix = embedding_matrix[:max_length, :]\n",
    "    return np.sum(embedding_matrix, axis=0)\n",
    "\n",
    "def train_word2vec(corpus, vector_size=100, window=10, min_count=1, workers=4, sg=1):\n",
    "    return Word2Vec(sentences=corpus, vector_size=vector_size, window=window, min_count=min_count, workers=workers, sg=sg)\n",
    "\n",
    "def train_fasttext(corpus, vector_size=100, window=10, min_count=1, workers=4, sg=1):\n",
    "    return FastText(sentences=corpus, vector_size=vector_size, window=window, min_count=min_count, workers=workers, sg=sg)\n",
    "\n",
    "def choose_embedding_model(corpus, model_type, glove_file):\n",
    "    if model_type == 'word2vec':\n",
    "        model = train_word2vec(corpus)\n",
    "    elif model_type == 'fasttext':\n",
    "        model = train_fasttext(corpus)\n",
    "    elif model_type == 'glove' and glove_file is not None:\n",
    "        model = load_glove(glove_file)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type or missing GloVe file.\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.iloc[:, -1] != 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus=data['text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_type = 'word2vec' \n",
    "embedding_model = choose_embedding_model(tokenized_corpus, model_type=embedding_type, glove_file=r'D:\\CODING\\Python\\NLP\\END_SEM\\glove\\hindi\\vectors.txt')\n",
    "data['embedding_matrix'] = data['text'].apply(lambda x: generate_embedding_matrix(x, embedding_model, embedding_type))\n",
    "X = np.array(data['embedding_matrix'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_one_hot = to_categorical(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=input_shape))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# BiLSTM (Bidirectional LSTM)\n",
    "def create_bilstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(128), input_shape=input_shape))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# GRU\n",
    "def create_gru_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(128, input_shape=input_shape))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# CNN-LSTM\n",
    "def create_cnn_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(64, 3, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# CNN-BiLSTM\n",
    "def create_cnn_bilstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(64, 3, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Bidirectional(LSTM(128)))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# CNN-GRU\n",
    "def create_cnn_gru_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(64, 3, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(GRU(128))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# LSTM with Attention\n",
    "def create_lstm_attention_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    lstm_out = LSTM(128, return_sequences=True)(inputs)\n",
    "    attention = tf.keras.layers.Attention()([lstm_out, lstm_out])\n",
    "    attention = tf.reduce_mean(attention, axis=1)\n",
    "    outputs = Dense(2, activation='softmax')(attention)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# CNN with Attention\n",
    "def create_cnn_attention_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    cnn_out = Conv1D(64, 3, activation='relu')(inputs)\n",
    "    cnn_out = MaxPooling1D(2)(cnn_out)\n",
    "    attention = tf.keras.layers.Attention()([cnn_out, cnn_out])\n",
    "    attention = tf.reduce_mean(attention, axis=1)\n",
    "    outputs = Dense(2, activation='softmax')(attention)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# BiLSTM with Attention\n",
    "def create_bilstm_attention_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    bilstm_out = Bidirectional(LSTM(128, return_sequences=True))(inputs)\n",
    "    attention = tf.keras.layers.Attention()([bilstm_out, bilstm_out])\n",
    "    attention = tf.reduce_mean(attention, axis=1)\n",
    "    outputs = Dense(2, activation='softmax')(attention)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# GRU with Attention\n",
    "def create_gru_attention_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    gru_out = GRU(128, return_sequences=True)(inputs)\n",
    "    attention = tf.keras.layers.Attention()([gru_out, gru_out])\n",
    "    attention = tf.reduce_mean(attention, axis=1)\n",
    "    outputs = Dense(2, activation='softmax')(attention)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM model...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ANACONDA\\envs\\tensorflow\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 - 3s - 49ms/step - accuracy: 0.5355 - loss: 0.6876\n",
      "Epoch 2/10\n",
      "52/52 - 2s - 30ms/step - accuracy: 0.5877 - loss: 0.6697\n",
      "Epoch 3/10\n",
      "52/52 - 2s - 30ms/step - accuracy: 0.6052 - loss: 0.6659\n",
      "Epoch 4/10\n",
      "52/52 - 2s - 30ms/step - accuracy: 0.6178 - loss: 0.6614\n",
      "Epoch 5/10\n",
      "52/52 - 2s - 30ms/step - accuracy: 0.6010 - loss: 0.6638\n",
      "Epoch 6/10\n",
      "52/52 - 2s - 30ms/step - accuracy: 0.6148 - loss: 0.6636\n",
      "Epoch 7/10\n",
      "52/52 - 2s - 30ms/step - accuracy: 0.6232 - loss: 0.6569\n",
      "Epoch 8/10\n",
      "52/52 - 2s - 30ms/step - accuracy: 0.6208 - loss: 0.6555\n",
      "Epoch 9/10\n",
      "52/52 - 2s - 29ms/step - accuracy: 0.6100 - loss: 0.6570\n",
      "Epoch 10/10\n",
      "52/52 - 2s - 29ms/step - accuracy: 0.6190 - loss: 0.6545\n",
      "LSTM Test Accuracy: 0.5625\n",
      "Training BiLSTM model...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ANACONDA\\envs\\tensorflow\\Lib\\site-packages\\keras\\src\\layers\\rnn\\bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 - 4s - 72ms/step - accuracy: 0.5895 - loss: 0.6697\n",
      "Epoch 2/10\n",
      "52/52 - 2s - 35ms/step - accuracy: 0.6142 - loss: 0.6570\n",
      "Epoch 3/10\n",
      "52/52 - 2s - 34ms/step - accuracy: 0.5986 - loss: 0.6720\n",
      "Epoch 4/10\n",
      "52/52 - 2s - 34ms/step - accuracy: 0.6148 - loss: 0.6597\n",
      "Epoch 5/10\n",
      "52/52 - 2s - 34ms/step - accuracy: 0.6154 - loss: 0.6591\n",
      "Epoch 6/10\n",
      "52/52 - 2s - 34ms/step - accuracy: 0.6226 - loss: 0.6555\n",
      "Epoch 7/10\n",
      "52/52 - 2s - 34ms/step - accuracy: 0.6124 - loss: 0.6559\n",
      "Epoch 8/10\n",
      "52/52 - 6s - 119ms/step - accuracy: 0.6202 - loss: 0.6538\n",
      "Epoch 9/10\n",
      "52/52 - 6s - 109ms/step - accuracy: 0.6316 - loss: 0.6530\n",
      "Epoch 10/10\n",
      "52/52 - 3s - 60ms/step - accuracy: 0.6328 - loss: 0.6516\n",
      "BiLSTM Test Accuracy: 0.588942289352417\n",
      "Training GRU model...\n",
      "Epoch 1/10\n",
      "52/52 - 3s - 59ms/step - accuracy: 0.5637 - loss: 0.6850\n",
      "Epoch 2/10\n",
      "52/52 - 2s - 30ms/step - accuracy: 0.6184 - loss: 0.6661\n",
      "Epoch 3/10\n",
      "52/52 - 2s - 30ms/step - accuracy: 0.6112 - loss: 0.6621\n",
      "Epoch 4/10\n",
      "52/52 - 2s - 30ms/step - accuracy: 0.6232 - loss: 0.6586\n",
      "Epoch 5/10\n",
      "52/52 - 2s - 29ms/step - accuracy: 0.6040 - loss: 0.6642\n",
      "Epoch 6/10\n",
      "52/52 - 2s - 30ms/step - accuracy: 0.6238 - loss: 0.6561\n",
      "Epoch 7/10\n",
      "52/52 - 2s - 30ms/step - accuracy: 0.6136 - loss: 0.6577\n",
      "Epoch 8/10\n",
      "52/52 - 2s - 29ms/step - accuracy: 0.6310 - loss: 0.6546\n",
      "Epoch 9/10\n",
      "52/52 - 2s - 30ms/step - accuracy: 0.6136 - loss: 0.6575\n",
      "Epoch 10/10\n",
      "52/52 - 2s - 30ms/step - accuracy: 0.6148 - loss: 0.6544\n",
      "GRU Test Accuracy: 0.6177884340286255\n",
      "Training CNN-LSTM model...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ANACONDA\\envs\\tensorflow\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 - 2s - 42ms/step - accuracy: 0.5811 - loss: 0.6764\n",
      "Epoch 2/10\n",
      "52/52 - 1s - 18ms/step - accuracy: 0.6016 - loss: 0.6703\n",
      "Epoch 3/10\n",
      "52/52 - 1s - 18ms/step - accuracy: 0.6262 - loss: 0.6570\n",
      "Epoch 4/10\n",
      "52/52 - 1s - 18ms/step - accuracy: 0.6238 - loss: 0.6538\n",
      "Epoch 5/10\n",
      "52/52 - 1s - 18ms/step - accuracy: 0.6166 - loss: 0.6551\n",
      "Epoch 6/10\n",
      "52/52 - 1s - 19ms/step - accuracy: 0.6322 - loss: 0.6478\n",
      "Epoch 7/10\n",
      "52/52 - 1s - 18ms/step - accuracy: 0.6364 - loss: 0.6476\n",
      "Epoch 8/10\n",
      "52/52 - 1s - 18ms/step - accuracy: 0.6238 - loss: 0.6455\n",
      "Epoch 9/10\n",
      "52/52 - 1s - 18ms/step - accuracy: 0.6280 - loss: 0.6512\n",
      "Epoch 10/10\n",
      "52/52 - 1s - 18ms/step - accuracy: 0.6316 - loss: 0.6465\n",
      "CNN-LSTM Test Accuracy: 0.620192289352417\n",
      "Training CNN-BiLSTM model...\n",
      "Epoch 1/10\n",
      "52/52 - 3s - 63ms/step - accuracy: 0.5457 - loss: 0.6951\n",
      "Epoch 2/10\n",
      "52/52 - 1s - 23ms/step - accuracy: 0.6028 - loss: 0.6690\n",
      "Epoch 3/10\n",
      "52/52 - 1s - 22ms/step - accuracy: 0.6070 - loss: 0.6640\n",
      "Epoch 4/10\n",
      "52/52 - 1s - 23ms/step - accuracy: 0.6214 - loss: 0.6512\n",
      "Epoch 5/10\n",
      "52/52 - 1s - 22ms/step - accuracy: 0.6136 - loss: 0.6614\n",
      "Epoch 6/10\n",
      "52/52 - 1s - 22ms/step - accuracy: 0.6286 - loss: 0.6513\n",
      "Epoch 7/10\n",
      "52/52 - 1s - 23ms/step - accuracy: 0.6052 - loss: 0.6630\n",
      "Epoch 8/10\n",
      "52/52 - 1s - 22ms/step - accuracy: 0.6196 - loss: 0.6483\n",
      "Epoch 9/10\n",
      "52/52 - 1s - 22ms/step - accuracy: 0.6244 - loss: 0.6522\n",
      "Epoch 10/10\n",
      "52/52 - 1s - 22ms/step - accuracy: 0.6346 - loss: 0.6424\n",
      "CNN-BiLSTM Test Accuracy: 0.5793269276618958\n",
      "Training CNN-GRU model...\n",
      "Epoch 1/10\n",
      "52/52 - 2s - 43ms/step - accuracy: 0.5607 - loss: 0.6883\n",
      "Epoch 2/10\n",
      "52/52 - 1s - 18ms/step - accuracy: 0.5944 - loss: 0.6692\n",
      "Epoch 3/10\n",
      "52/52 - 1s - 19ms/step - accuracy: 0.6034 - loss: 0.6702\n",
      "Epoch 4/10\n",
      "52/52 - 1s - 17ms/step - accuracy: 0.6262 - loss: 0.6568\n",
      "Epoch 5/10\n",
      "52/52 - 1s - 17ms/step - accuracy: 0.6334 - loss: 0.6538\n",
      "Epoch 6/10\n",
      "52/52 - 1s - 18ms/step - accuracy: 0.6406 - loss: 0.6464\n",
      "Epoch 7/10\n",
      "52/52 - 1s - 18ms/step - accuracy: 0.6340 - loss: 0.6490\n",
      "Epoch 8/10\n",
      "52/52 - 1s - 18ms/step - accuracy: 0.6214 - loss: 0.6467\n",
      "Epoch 9/10\n",
      "52/52 - 1s - 18ms/step - accuracy: 0.6280 - loss: 0.6454\n",
      "Epoch 10/10\n",
      "52/52 - 1s - 17ms/step - accuracy: 0.6292 - loss: 0.6464\n",
      "CNN-GRU Test Accuracy: 0.6081730723381042\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"LSTM\": create_lstm_model,\n",
    "    \"BiLSTM\": create_bilstm_model,\n",
    "    \"GRU\": create_gru_model,\n",
    "    \"CNN-LSTM\": create_cnn_lstm_model,\n",
    "    \"CNN-BiLSTM\": create_cnn_bilstm_model,\n",
    "    \"CNN-GRU\": create_cnn_gru_model,\n",
    "    # \"LSTM-Attention\": create_lstm_attention_model,\n",
    "    # \"CNN-Attention\": create_cnn_attention_model,\n",
    "    # \"BiLSTM-Attention\": create_bilstm_attention_model,\n",
    "    # \"GRU-Attention\": create_gru_attention_model\n",
    "}\n",
    "\n",
    "input_shape = (X_train.shape[1], 1)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, create_model in models.items():\n",
    "    model = create_model(input_shape)\n",
    "    print(f\"Training {name} model...\")\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=2)\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    results[name] = score\n",
    "    print(f\"{name} Test Accuracy: {score[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
