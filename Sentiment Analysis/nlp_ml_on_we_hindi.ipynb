{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5684fcc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from lazypredict.Supervised import LazyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60441712",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(r'D:\\CODING\\Python\\NLP\\END_SEM\\hindi_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5076b52c-8d54-4988-a10d-a042ecf711a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>लोग वतन तक खा जाते हैं इसका इसे यकीन नहींमान ज...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>गुमनाम है वतन पर मिटने वाले लोग आतन्कवादियों स...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ज़ंजीर बदली जा रही थी मैं समझा था रिहाई हो गयी है</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>यूपी में बड़े स्तर पर दंगे करवा सकती है बीजेपी...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>अंग्रेजी नहीं आती है इसलिए हिन्दी ट्विट ज्यादा...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0  लोग वतन तक खा जाते हैं इसका इसे यकीन नहींमान ज...  negative\n",
       "1  गुमनाम है वतन पर मिटने वाले लोग आतन्कवादियों स...  negative\n",
       "2  ज़ंजीर बदली जा रही थी मैं समझा था रिहाई हो गयी है  negative\n",
       "3  यूपी में बड़े स्तर पर दंगे करवा सकती है बीजेपी...  negative\n",
       "4  अंग्रेजी नहीं आती है इसलिए हिन्दी ट्विट ज्यादा...  negative"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c239d0b-3281-475d-9b8c-b048af3b1d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4991814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_clean_hindi(text):\n",
    "    # Define the function to preprocess Hindi text by tokenizing\n",
    "    def preprocess_text_hindi(text):\n",
    "        tokens = text.split(\" \")\n",
    "        return tokens\n",
    "\n",
    "    # Define the function to remove non-Hindi characters while preserving spaces\n",
    "    def remove_non_hindi(text):\n",
    "        hindi_pattern = re.compile(\"[\\u0900-\\u097F\\s]+\")  # Unicode range for Hindi characters and space\n",
    "        hindi_text = hindi_pattern.findall(text)\n",
    "        cleaned_text = ''.join(hindi_text)\n",
    "        return cleaned_text\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = preprocess_text_hindi(text)\n",
    "\n",
    "    # Remove non-Hindi characters while preserving spaces\n",
    "    cleaned_text = ' '.join([remove_non_hindi(token) for token in tokens])\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "data['text'] = data['text'].apply(preprocess_and_clean_hindi)\n",
    "\n",
    "data['label'] = data['sentiment'].map({'positive': 1, 'negative': 0,'neutral': 2})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0b8b625-5685-4469-b9ea-8446708bb019",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['label'] != 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41ec762c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding_vector(word_vector, word2vec_model):\n",
    "    word_embeddings = []\n",
    "    for word in word_vector:\n",
    "        if word in word2vec_model.wv:\n",
    "            word_embedding = word2vec_model.wv[word]\n",
    "            word_embeddings.append(word_embedding)\n",
    "\n",
    "    embedding_vector = np.sum(word_embeddings, axis=0)\n",
    "    return embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f007c8a-540c-44b7-a10b-7f0865aa97c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus=data['text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1e0a3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b226e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['embedding_vector'] = data['text'].apply(lambda x: generate_embedding_vector(x, word2vec_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4f5dc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(data['embedding_vector'].tolist())\n",
    "y = np.array(data['label'].tolist())\n",
    "\n",
    "del data\n",
    "del tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60851afd-1460-441e-bd0f-6b8a7bb2aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdb2f7c0-da43-482f-b02f-02447c81bc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 28/29 [00:08<00:00,  4.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 918, number of negative: 746\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001461 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 25485\n",
      "[LightGBM] [Info] Number of data points in the train set: 1664, number of used features: 100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.551683 -> initscore=0.207472\n",
      "[LightGBM] [Info] Start training from score 0.207472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:08<00:00,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
      "Model                                                                           \n",
      "LinearSVC                          0.63               0.63     0.63      0.63   \n",
      "BaggingClassifier                  0.63               0.63     0.63      0.63   \n",
      "LGBMClassifier                     0.62               0.62     0.62      0.62   \n",
      "RidgeClassifier                    0.62               0.62     0.62      0.62   \n",
      "LogisticRegression                 0.62               0.62     0.62      0.62   \n",
      "LinearDiscriminantAnalysis         0.62               0.62     0.62      0.61   \n",
      "RandomForestClassifier             0.62               0.62     0.62      0.61   \n",
      "NuSVC                              0.61               0.61     0.61      0.61   \n",
      "SVC                                0.61               0.61     0.61      0.60   \n",
      "ExtraTreesClassifier               0.61               0.61     0.61      0.61   \n",
      "RidgeClassifierCV                  0.61               0.61     0.61      0.60   \n",
      "CalibratedClassifierCV             0.60               0.60     0.60      0.59   \n",
      "XGBClassifier                      0.60               0.60     0.60      0.60   \n",
      "DecisionTreeClassifier             0.60               0.60     0.60      0.60   \n",
      "LabelPropagation                   0.57               0.57     0.57      0.57   \n",
      "LabelSpreading                     0.57               0.57     0.57      0.57   \n",
      "AdaBoostClassifier                 0.57               0.57     0.57      0.57   \n",
      "Perceptron                         0.56               0.56     0.56      0.56   \n",
      "KNeighborsClassifier               0.56               0.56     0.56      0.56   \n",
      "SGDClassifier                      0.56               0.56     0.56      0.56   \n",
      "QuadraticDiscriminantAnalysis      0.55               0.55     0.55      0.46   \n",
      "ExtraTreeClassifier                0.53               0.53     0.53      0.53   \n",
      "NearestCentroid                    0.51               0.51     0.51      0.51   \n",
      "PassiveAggressiveClassifier        0.51               0.51     0.51      0.51   \n",
      "GaussianNB                         0.51               0.51     0.51      0.51   \n",
      "DummyClassifier                    0.50               0.50     0.50      0.33   \n",
      "BernoulliNB                        0.50               0.50     0.50      0.49   \n",
      "\n",
      "                               Time Taken  \n",
      "Model                                      \n",
      "LinearSVC                            0.31  \n",
      "BaggingClassifier                    1.63  \n",
      "LGBMClassifier                       0.21  \n",
      "RidgeClassifier                      0.02  \n",
      "LogisticRegression                   0.04  \n",
      "LinearDiscriminantAnalysis           0.09  \n",
      "RandomForestClassifier               2.11  \n",
      "NuSVC                                0.46  \n",
      "SVC                                  0.18  \n",
      "ExtraTreesClassifier                 0.48  \n",
      "RidgeClassifierCV                    0.09  \n",
      "CalibratedClassifierCV               0.22  \n",
      "XGBClassifier                        0.45  \n",
      "DecisionTreeClassifier               0.25  \n",
      "LabelPropagation                     0.07  \n",
      "LabelSpreading                       0.11  \n",
      "AdaBoostClassifier                   1.48  \n",
      "Perceptron                           0.02  \n",
      "KNeighborsClassifier                 0.22  \n",
      "SGDClassifier                        0.04  \n",
      "QuadraticDiscriminantAnalysis        0.05  \n",
      "ExtraTreeClassifier                  0.02  \n",
      "NearestCentroid                      0.02  \n",
      "PassiveAggressiveClassifier          0.02  \n",
      "GaussianNB                           0.03  \n",
      "DummyClassifier                      0.01  \n",
      "BernoulliNB                          0.08  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25dfa2e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accuracy            0.63\n",
       "Balanced Accuracy   0.63\n",
       "ROC AUC             0.63\n",
       "F1 Score            0.63\n",
       "Time Taken          0.31\n",
       "Name: LinearSVC, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the best model from the list of models\n",
    "\n",
    "best_model = models.iloc[0]\n",
    "best_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
