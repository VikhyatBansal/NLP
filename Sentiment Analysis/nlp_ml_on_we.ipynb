{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5684fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from lazypredict.Supervised import LazyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60441712",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(r'D:\\CODING\\Python\\NLP\\END_SEM\\IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4991814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "data['review'] = data['review'].apply(preprocess_text)\n",
    "\n",
    "data['label'] = data['sentiment'].map({'positive': 1, 'negative': 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41ec762c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding_vector(word_vector, word2vec_model):\n",
    "    word_embeddings = []\n",
    "    for word in word_vector:\n",
    "        if word in word2vec_model.wv:\n",
    "            word_embedding = word2vec_model.wv[word]\n",
    "            word_embeddings.append(word_embedding)\n",
    "\n",
    "    embedding_vector = np.sum(word_embeddings, axis=0)\n",
    "    return embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63a89ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus=data['review'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1e0a3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b226e875",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['embedding_vector'] = data['review'].apply(lambda x: generate_embedding_vector(x, word2vec_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4f5dc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(data['embedding_vector'].tolist())\n",
    "y = np.array(data['label'].tolist())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "del data\n",
    "del tokenized_corpus\n",
    "del X\n",
    "del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29f5ccc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 28/29 [12:14<00:24, 24.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 19961, number of negative: 20039\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006412 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499025 -> initscore=-0.003900\n",
      "[LightGBM] [Info] Start training from score -0.003900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [12:15<00:00, 25.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
      "Model                                                                           \n",
      "SVC                                0.87               0.87     0.87      0.87   \n",
      "LogisticRegression                 0.86               0.86     0.86      0.86   \n",
      "CalibratedClassifierCV             0.86               0.86     0.86      0.86   \n",
      "LinearDiscriminantAnalysis         0.86               0.86     0.86      0.86   \n",
      "RidgeClassifier                    0.86               0.86     0.86      0.86   \n",
      "RidgeClassifierCV                  0.86               0.86     0.86      0.86   \n",
      "SGDClassifier                      0.85               0.85     0.85      0.85   \n",
      "LinearSVC                          0.85               0.85     0.85      0.85   \n",
      "XGBClassifier                      0.85               0.85     0.85      0.85   \n",
      "LGBMClassifier                     0.84               0.84     0.84      0.84   \n",
      "NuSVC                              0.84               0.84     0.84      0.84   \n",
      "RandomForestClassifier             0.83               0.83     0.83      0.83   \n",
      "ExtraTreesClassifier               0.83               0.83     0.83      0.83   \n",
      "AdaBoostClassifier                 0.81               0.81     0.81      0.81   \n",
      "BaggingClassifier                  0.80               0.80     0.80      0.80   \n",
      "KNeighborsClassifier               0.79               0.79     0.79      0.79   \n",
      "Perceptron                         0.79               0.79     0.79      0.79   \n",
      "PassiveAggressiveClassifier        0.77               0.77     0.77      0.77   \n",
      "QuadraticDiscriminantAnalysis      0.77               0.77     0.77      0.76   \n",
      "NearestCentroid                    0.74               0.74     0.74      0.74   \n",
      "DecisionTreeClassifier             0.73               0.73     0.73      0.73   \n",
      "BernoulliNB                        0.72               0.72     0.72      0.72   \n",
      "GaussianNB                         0.70               0.70     0.70      0.69   \n",
      "ExtraTreeClassifier                0.68               0.68     0.68      0.68   \n",
      "LabelSpreading                     0.51               0.52     0.52      0.37   \n",
      "LabelPropagation                   0.51               0.52     0.52      0.37   \n",
      "DummyClassifier                    0.50               0.50     0.50      0.33   \n",
      "\n",
      "                               Time Taken  \n",
      "Model                                      \n",
      "SVC                                119.46  \n",
      "LogisticRegression                   0.83  \n",
      "CalibratedClassifierCV               6.31  \n",
      "LinearDiscriminantAnalysis           0.47  \n",
      "RidgeClassifier                      0.17  \n",
      "RidgeClassifierCV                    0.54  \n",
      "SGDClassifier                        0.68  \n",
      "LinearSVC                           14.15  \n",
      "XGBClassifier                        1.44  \n",
      "LGBMClassifier                       0.96  \n",
      "NuSVC                              190.66  \n",
      "RandomForestClassifier              74.13  \n",
      "ExtraTreesClassifier                11.07  \n",
      "AdaBoostClassifier                  49.37  \n",
      "BaggingClassifier                   89.74  \n",
      "KNeighborsClassifier                 0.98  \n",
      "Perceptron                           0.26  \n",
      "PassiveAggressiveClassifier          0.30  \n",
      "QuadraticDiscriminantAnalysis        0.32  \n",
      "NearestCentroid                      0.17  \n",
      "DecisionTreeClassifier              11.62  \n",
      "BernoulliNB                          0.21  \n",
      "GaussianNB                           0.19  \n",
      "ExtraTreeClassifier                  0.22  \n",
      "LabelSpreading                     120.05  \n",
      "LabelPropagation                    40.43  \n",
      "DummyClassifier                      0.12  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aec31c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accuracy              0.87\n",
       "Balanced Accuracy     0.87\n",
       "ROC AUC               0.87\n",
       "F1 Score              0.87\n",
       "Time Taken          119.46\n",
       "Name: SVC, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = models.iloc[0]\n",
    "best_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
