{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense,AveragePooling1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, GRU, Conv1D, MaxPooling1D, Flatten, Attention, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(r'D:\\CODING\\Python\\NLP\\END_SEM\\IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "data['review'] = data['review'].apply(preprocess_text)\n",
    "\n",
    "data['label'] = data['sentiment'].map({'positive': 1, 'negative': 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(glove_file):\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        word_embeddings = {}\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            word_embeddings[word] = vector\n",
    "    return word_embeddings\n",
    "\n",
    "def generate_embedding_matrix(word_vector, embedding_model, embedding_type, max_length=200):\n",
    "    if embedding_type == 'glove':\n",
    "        embedding_dim = len(next(iter(embedding_model.values())))\n",
    "        embedding_matrix = np.zeros((max_length, embedding_dim))\n",
    "        for i, word in enumerate(word_vector):\n",
    "            if word in embedding_model:\n",
    "                embedding_matrix[i] = embedding_model[word]\n",
    "            if i >= max_length - 1:\n",
    "                break\n",
    "    else:\n",
    "        word_embeddings = []\n",
    "        for word in word_vector:\n",
    "            if word in embedding_model.wv:\n",
    "                word_embedding = embedding_model.wv[word]\n",
    "                word_embeddings.append(word_embedding)\n",
    "        embedding_matrix = np.array(word_embeddings)\n",
    "        if len(embedding_matrix) < max_length:\n",
    "            embedding_matrix = np.pad(embedding_matrix, ((0, max_length - len(embedding_matrix)), (0, 0)), mode='constant')\n",
    "        else:\n",
    "            embedding_matrix = embedding_matrix[:max_length, :]\n",
    "    return np.sum(embedding_matrix, axis=0)\n",
    "\n",
    "def train_word2vec(corpus, vector_size=100, window=10, min_count=1, workers=4, sg=1):\n",
    "    return Word2Vec(sentences=corpus, vector_size=vector_size, window=window, min_count=min_count, workers=workers, sg=sg)\n",
    "\n",
    "def train_fasttext(corpus, vector_size=100, window=10, min_count=1, workers=4, sg=1):\n",
    "    return FastText(sentences=corpus, vector_size=vector_size, window=window, min_count=min_count, workers=workers, sg=sg)\n",
    "\n",
    "def choose_embedding_model(corpus, model_type, glove_file):\n",
    "    if model_type == 'word2vec':\n",
    "        model = train_word2vec(corpus)\n",
    "    elif model_type == 'fasttext':\n",
    "        model = train_fasttext(corpus)\n",
    "    elif model_type == 'glove' and glove_file is not None:\n",
    "        model = load_glove(glove_file)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type or missing GloVe file.\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus=data['review'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_type = 'glove' \n",
    "embedding_model = choose_embedding_model(tokenized_corpus, model_type=embedding_type, glove_file=r'D:\\CODING\\Python\\NLP\\END_SEM\\glove\\eng\\glove.6B.100d.txt')\n",
    "data['embedding_matrix'] = data['review'].apply(lambda x: generate_embedding_matrix(x, embedding_model, embedding_type))\n",
    "X = np.array(data['embedding_matrix'].tolist())\n",
    "y=data['label'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_one_hot = to_categorical(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=input_shape))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# BiLSTM (Bidirectional LSTM)\n",
    "def create_bilstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(128), input_shape=input_shape))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# GRU\n",
    "def create_gru_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(128, input_shape=input_shape))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# CNN-LSTM\n",
    "def create_cnn_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(64, 3, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# CNN-BiLSTM\n",
    "def create_cnn_bilstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(64, 3, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Bidirectional(LSTM(128)))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# CNN-GRU\n",
    "def create_cnn_gru_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(64, 3, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(GRU(128))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# LSTM with Attention\n",
    "def create_lstm_attention_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    lstm_out = LSTM(128, return_sequences=True)(inputs)\n",
    "    attention = tf.keras.layers.Attention()([lstm_out, lstm_out])\n",
    "    attention = tf.reduce_mean(attention, axis=1)\n",
    "    outputs = Dense(2, activation='softmax')(attention)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# CNN with Attention\n",
    "def create_cnn_attention_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    cnn_out = Conv1D(64, 3, activation='relu')(inputs)\n",
    "    cnn_out = MaxPooling1D(2)(cnn_out)\n",
    "    attention = tf.keras.layers.Attention()([cnn_out, cnn_out])\n",
    "    attention = tf.reduce_mean(attention, axis=1)\n",
    "    outputs = Dense(2, activation='softmax')(attention)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# BiLSTM with Attention\n",
    "def create_bilstm_attention_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    bilstm_out = Bidirectional(LSTM(128, return_sequences=True))(inputs)\n",
    "    attention = tf.keras.layers.Attention()([bilstm_out, bilstm_out])\n",
    "    attention = tf.reduce_mean(attention, axis=1)\n",
    "    outputs = Dense(2, activation='softmax')(attention)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# GRU with Attention\n",
    "def create_gru_attention_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    gru_out = GRU(128, return_sequences=True)(inputs)\n",
    "    attention = tf.keras.layers.Attention()([gru_out, gru_out])\n",
    "    attention = tf.reduce_mean(attention, axis=1)\n",
    "    outputs = Dense(2, activation='softmax')(attention)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"LSTM\": create_lstm_model,\n",
    "    \"BiLSTM\": create_bilstm_model,\n",
    "    \"GRU\": create_gru_model,\n",
    "    \"CNN-LSTM\": create_cnn_lstm_model,\n",
    "    \"CNN-BiLSTM\": create_cnn_bilstm_model,\n",
    "    \"CNN-GRU\": create_cnn_gru_model,\n",
    "    # \"LSTM-Attention\": create_lstm_attention_model,\n",
    "    # \"CNN-Attention\": create_cnn_attention_model,\n",
    "    # \"BiLSTM-Attention\": create_bilstm_attention_model,\n",
    "    # \"GRU-Attention\": create_gru_attention_model\n",
    "}\n",
    "\n",
    "input_shape = (X_train.shape[1], 1)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, create_model in models.items():\n",
    "    model = create_model(input_shape)\n",
    "    print(f\"Training {name} model...\")\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=2)\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    results[name] = score\n",
    "    print(f\"{name} Test Accuracy: {score[1]}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
